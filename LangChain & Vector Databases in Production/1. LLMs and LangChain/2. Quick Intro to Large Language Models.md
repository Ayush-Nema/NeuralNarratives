### **Table of contents**
- [[#Introduction|Introduction]]
- [[#LLMs in general|LLMs in general]]
- [[#Tokens distributions and predicting the next token|Tokens distributions and predicting the next token]]
		- [[#Tracking token usage|Tracking token usage]]
- [[#Few-shot learning|Few-shot learning]]
- [[#Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering|Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering]]
		- [[#Setting up the environment|Setting up the environment]]
		- [[#Creating a question-answering prompt template|Creating a question-answering prompt template]]
		- [[#Asking multiple questions|Asking multiple questions]]
		- [[#Text summarization|Text summarization]]
		- [[#Text translation|Text translation]]

Notebook link: [Click here](https://colab.research.google.com/drive/1DRpBmq6i8fREqIWItHdpjECdYoPWNrIP?usp=sharing)

---
# Introduction
- Quick introduction to the inner workings of GPT-3 and GPT-4, focusing on their few-shot learning capabilities, emergent abilities, and the scaling laws. 
- Dive into some easy-to-understand examples of how these models excel in tasks such as text summarization and translation just by providing a few examples without the need for fine-tuning.
- Discuss some of the potential pitfalls, including hallucinations and biases, which can lead to inaccurate or misleading outputs.


# LLMs in general
- LLMs are deep learning models with billions of parameters that excel at a wide range of natural language processing tasks. 
- Can perform tasks like translation, sentiment analysis, and chatbot conversations without being specifically trained for them. 
- Can be used without fine-tuning by employing "prompting" techniques, where a question is presented as a text prompt with examples of similar problems and solutions.

**Architecture**
LLMs typically consist of multiple layers of neural networks, feedforward layers, embedding layers, and attention layers.

**Maximum number of tokens**
- In the LangChain library, the LLM context size, or the maximum number of tokens the model can process, is determined by the specific implementation of the LLM. 
- In the case of the OpenAI implementation in LangChain, the maximum number of tokens is defined by the underlying OpenAI model being used. 
- To find the maximum number of tokens for the OpenAI model, refer to the `max_tokens` attribute provided on the OpenAI [documentation](https://platform.openai.com/docs/models/gpt-4) or API. 
- For example, if you’re using the `GPT-3` model, the maximum number of tokens supported by the model is 2,049.
- Issue with context length:
	- ensure that the input text does not exceed the maximum number of tokens supported by the model, as this may result in truncation or errors during processing. 
	- To handle this, you can split the input text into smaller chunks and process them separately, making sure that each chunk is within the allowed token limit. You can then combine the results as needed (see below pseudo code).

```python
from langchain.llms import OpenAI

# ==== PSEUDO CODE ====
llm = OpenAI(model_name="text-davinci-003")

# Define the input text
input_text = "your_long_input_text"

# Determine the maximum number of tokens from documentation
max_tokens = 4097

# Split the input text into chunks based on the max tokens
text_chunks = split_text_into_chunks(input_text, max_tokens)

# Process each chunk separately
results = []
for chunk in text_chunks:
    result = llm.process(chunk)
    results.append(result)

# Combine the results as needed
final_result = combine_results(results)
```

# Tokens distributions and predicting the next token
### Tracking token usage
```python
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback

llm = OpenAI(model_name="text-davinci-003", n=2, best_of=2)

with get_openai_callback() as cb:
    result = llm("Tell me a joke")
    print(cb)
```

Output
```
Tokens Used: 44
	Prompt Tokens: 4
	Completion Tokens: 40
Successful Requests: 1
Total Cost (USD): $0.00088
```


# Few-shot learning
**Few-shot learning** is a remarkable ability that allows LLMs to learn and generalize from limited examples. With LangChain, examples can be hard-coded, but dynamically selecting them often proves more powerful, enabling LLMs to adapt and tackle tasks with minimal training data swiftly.

This approach involves using the `FewShotPromptTemplate` class, which takes in a `PromptTemplate` and a list of a few shot examples. We can streamline this process by utilizing LangChain's FewShotPromptTemplate to structure the approach:
```python
from langchain import PromptTemplate
from langchain import FewShotPromptTemplate

# create our examples
examples = [
    {
        "query": "What's the weather like?",
        "answer": "It's raining cats and dogs, better bring an umbrella!"
    }, {
        "query": "How old are you?",
        "answer": "Age is just a number, but I'm timeless."
    }
]

# create an example template
example_template = """
User: {query}
AI: {answer}
"""

# create a prompt example from above template
example_prompt = PromptTemplate(
    input_variables=["query", "answer"],
    template=example_template
)

# now break our previous prompt into a prefix and suffix
# the prefix is our instructions
prefix = """The following are excerpts from conversations with an AI
assistant. The assistant is known for its humor and wit, providing
entertaining and amusing responses to users' questions. Here are some
examples:
"""
# and the suffix our user input and output indicator
suffix = """
User: {query}
AI: """

# now create the few-shot prompt template
few_shot_prompt_template = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=prefix,
    suffix=suffix,
    input_variables=["query"],
    example_separator="\n\n"
)
```

After creating a template, we pass the example and user query, and we get the results
```python
from langchain.chat_models import ChatOpenAI
from langchain import LLMChain

# load the model
chat = ChatOpenAI(model_name="gpt-4", temperature=0.0)

chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)
chain.run("What's the meaning of life?")
```

Output:
```
To live life to the fullest and enjoy the journey!
```

# Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering
### Setting up the environment
To begin, we will need to install the `huggingface_hub` library in addition to previously installed packages and dependencies. Also, create the Huggingface API Key by navigating to Access Tokens page under the account’s Settings. The key must be set as an environment variable with `HUGGINGFACEHUB_API_TOKEN` key.
```python
!pip install -q huggingface_hub
```
### Creating a question-answering prompt template
Let's create a simple question-answering prompt template using LangChain.
```python
from langchain import PromptTemplate

template = """Question: {question}
Answer: """

prompt = PromptTemplate(
    template=template,
    input_variables=['question']
)

# user question
question = "What is the capital city of France?"
```

Next, we will use the Hugging Face model `google/flan-t5-large` to answer the question. The `HuggingfaceHub` class will connect to Hugging Face’s inference API and load the specified model.
```python
from langchain import HuggingFaceHub, LLMChain

# initialize Hub LLM
hub_llm = HuggingFaceHub(
        repo_id='google/flan-t5-large',
    model_kwargs={'temperature':0}
)

# create prompt template > LLM chain
llm_chain = LLMChain(
    prompt=prompt,
    llm=hub_llm
)

# ask the user question about the capital of France
print(llm_chain.run(question))
```
Output:
```
paris
```

We can also modify the **prompt template** to include multiple questions.

### Asking multiple questions
To ask multiple questions, we can either iterate through all questions one at a time or place all questions into a single prompt for more advanced LLMs. Let's start with the first approach:
```python
qa = [
    {'question': "What is the capital city of France?"},
    {'question': "What is the largest mammal on Earth?"},
    {'question': "Which gas is most abundant in Earth's atmosphere?"},
    {'question': "What color is a ripe banana?"}
]
res = llm_chain.generate(qa)
print( res )
```

Output:
```python
LLMResult(generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]], llm_output=None)
```
We can modify our prompt template to include multiple questions to implement a second approach. The language model will understand that we have multiple questions and answer them sequentially. This method performs best on more capable models.
```python
multi_template = """Answer the following questions one at a time.

Questions:
{questions}

Answers:
"""
long_prompt = PromptTemplate(template=multi_template, input_variables=["questions"])

llm_chain = LLMChain(
    prompt=long_prompt,
    llm=llm
)

qs_str = (
    "What is the capital city of France?\n" +
    "What is the largest mammal on Earth?\n" +
    "Which gas is most abundant in Earth's atmosphere?\n" +
	"What color is a ripe banana?\n"
)
llm_chain.run(qs_str)
```

Output:
```
1. The capital city of France is Paris.
2. The largest mammal on Earth is the blue whale.
3. The gas that is most abundant in Earth's atmosphere is nitrogen.
4. A ripe banana is yellow.
```

### Text summarization
Using LangChain, we can create a chain for text summarization. First, we need to set up the necessary imports and an instance of the OpenAI language model
```python
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
```

Next, we define a prompt template for summarization:
```python
summarization_template = "Summarize the following text to one sentence: {text}"
summarization_prompt = PromptTemplate(input_variables=["text"], template=summarization_template)
summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)
```

To use the summarization chain, simply call the `predict` method with the text to be summarized:
```python
text = "LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes."
summarized_text = summarization_chain.predict(text=text)
```

Output:
```
LangChain offers various modules for developing language model applications, which can be used alone for simple applications or combined for more complex ones.
```

### Text translation
It is one of the great attributes of Large Language models that enables them to perform multiple tasks just by changing the prompt. We use the same `llm` variable as defined before. However, pass a different prompt that asks for translating the query from a `source_language` to the `target_language`.
```python
translation_template = "Translate the following text from {source_language} to {target_language}: {text}"
translation_prompt = PromptTemplate(input_variables=["source_language", "target_language", "text"], template=translation_template)
translation_chain = LLMChain(llm=llm, prompt=translation_prompt)
```

To use the translation chain, call the `predict` method with the source language, target language, and text to be translated:
```python
source_language = "English"
target_language = "French"
text = "Your text here"
translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)
```

Output:
```python
Votre texte ici
```
